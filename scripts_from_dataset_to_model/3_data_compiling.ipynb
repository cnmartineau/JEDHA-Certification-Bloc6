{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3_data_compiling\n",
    "\n",
    "# get cleaned dataframes created in 1_data_cleaning\n",
    "# set filters (number of words, number of frames per video, number of batches)\n",
    "# compile data into small batches\n",
    "# (since videos do not all have the same length, pad videos shorter than desired frame number with zeros)\n",
    "# join batches\n",
    "# save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters\n",
    "\n",
    "word_nb = 10\n",
    "frame_nb = 50\n",
    "batch_nb = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of files per word that meet frame_nb criteria\n",
    "\n",
    "# directory to be scanned\n",
    "path_from = \"/path_of_directory_where_cleaned_dataframes_were_saved/\"\n",
    "\n",
    "# initilise variables to store info and list of files\n",
    "data_word = []\n",
    "data_info = pd.DataFrame(columns = [\"word\", \"count\"])\n",
    "\n",
    "# scan the directory\n",
    "obj_file = os.scandir(path_from)\n",
    "\n",
    "count = 0\n",
    "\n",
    "# loop through files\n",
    "for entry_file in obj_file:\n",
    "    if entry_file.is_file() and entry_file.name.endswith(\".csv\"):\n",
    "        count += 1\n",
    "        print(count)\n",
    "\n",
    "        # open current data\n",
    "        data_current = pd.read_csv(entry_file.path)\n",
    "        \n",
    "        # check whether data is not empty and filter with frame number and get word\n",
    "        if (data_current.shape[0] > 0) & (data_current.shape[0] <= frame_nb):\n",
    "            data_word.append(entry_file.name.split(\"_\")[0])\n",
    "\n",
    "# count csv files per word\n",
    "words_unique = np.unique(data_word)\n",
    "for i in range(0,len(words_unique)):\n",
    "    data_info.loc[i,\"word\"] = words_unique[i]\n",
    "    count = 0\n",
    "    for j in range(0,len(data_word)):\n",
    "        if words_unique[i] == data_word[j]:\n",
    "            count += 1\n",
    "    data_info.loc[i,\"count\"] = count\n",
    "\n",
    "# sort info\n",
    "data_info = data_info.sort_values(\"count\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visually check data info\n",
    "data_info.head(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word \"NS\" corresponds to geographical places, not taken further in our analysis\n",
    "# set word list based on data info\n",
    "\n",
    "word_list = [\"AUSSI\", \"LS\", \"OUI\", \"AVOIR\", \"SOURD\", \"QUOI\", \"MAIS\", \"NON\", \"PLUS.P\", \"REGARDER\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of files after filtering\n",
    "\n",
    "file_nb = 0\n",
    "# loop through data info\n",
    "for i in range(data_info.shape[0]):\n",
    "    if data_info.loc[i,\"word\"] in word_list:\n",
    "        file_nb += data_info.loc[i,\"count\"]\n",
    "\n",
    "print(file_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile data\n",
    "\n",
    "# directory to be scanned\n",
    "path_from = \"/path_of_directory_where_cleaned_dataframes_were_saved/\"\n",
    "\n",
    "# directory to save batches of data\n",
    "path_to = \"/path_of_directory_where_batches_will_be_saved/\"\n",
    "\n",
    "# initilise variables \n",
    "data_temp = np.zeros((frame_nb, 1659))\n",
    "labels_temp = [\"init_label\"] * frame_nb\n",
    "count_all = 0\n",
    "count = 0\n",
    "batch_id = 0\n",
    "\n",
    "# scan the directory\n",
    "obj_file = os.scandir(path_from)\n",
    "        \n",
    "# loop through files\n",
    "for entry_file in obj_file:\n",
    "    if entry_file.is_file() and entry_file.name.endswith(\".csv\"):\n",
    "        word = entry_file.name.split(\"_\")[0]\n",
    "\n",
    "        # process if word is in word list\n",
    "        if word in word_list:\n",
    "            data_current = pd.read_csv(entry_file.path)\n",
    "\n",
    "            # check number of frames\n",
    "            if (data_current.shape[0] > 0) & (data_current.shape[0] <= frame_nb):\n",
    "                count_all += 1\n",
    "                count += 1\n",
    "                print(count)  \n",
    "\n",
    "                # pad data to frame_nb\n",
    "                if data_current.shape[0] < frame_nb:\n",
    "                    pad_length = frame_nb - data_current.shape[0]\n",
    "                    pad = pd.DataFrame(0, index = range(pad_length), columns = data_current.columns)\n",
    "                    data_current = pd.concat([data_current, pad], axis = 0)\n",
    "\n",
    "                # compile data \n",
    "                data_current = data_current.to_numpy()\n",
    "                data_temp = np.vstack((data_temp,data_current))\n",
    "                labels_current = [word] * frame_nb\n",
    "                labels_temp = np.concatenate((labels_temp,labels_current), axis = 0)\n",
    "\n",
    "                # save when batch size is reached of when all files are processed\n",
    "                if (count == np.round((file_nb / batch_nb),0)) | (count_all == file_nb):\n",
    "                    batch_id += 1\n",
    "                    np.save(path_to + str(batch_id), data_temp)\n",
    "                    np.save(path_to + str(batch_id), labels_temp)\n",
    "\n",
    "                    count = 0\n",
    "                    data_temp = np.zeros((frame_nb, 1659))\n",
    "                    labels_temp = [\"init_label\"] * frame_nb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join batches of data\n",
    "\n",
    "# directory to be scanned\n",
    "path_from = \"/path_of_directory_where_batches_were_saved/\"\n",
    "\n",
    "# directory to save compiled data\n",
    "path_to = \"/path_of_directory_where_compiled_data_will_be_saved/\"\n",
    "\n",
    "# initilise variables \n",
    "data_all = np.zeros((50, 1659))\n",
    "labels_all = np.array([\"init_label\"] * 50)  \n",
    "\n",
    "# scan the directory\n",
    "obj_file = os.scandir(path_from)\n",
    "\n",
    "# loop through files\n",
    "for entry_file in obj_file:\n",
    "    if entry_file.is_file() and entry_file.name.startswith(\"data_temp\"):\n",
    "        data_current = np.load(entry_file.path)\n",
    "        data_all = np.vstack((data_all,data_current))\n",
    "\n",
    "    if entry_file.is_file() and entry_file.name.startswith(\"labels_temp\"):\n",
    "        labels_current = np.load(entry_file.path)\n",
    "        labels_all = np.concatenate((labels_all,labels_current))\n",
    "\n",
    "# drop columns that still contain nans\n",
    "columns_tokeep = []\n",
    "for j in range(data_all.shape[1]):\n",
    "    if np.sum(np.isnan(data_all[:,j])) == 0:\n",
    "        columns_tokeep.append(j)\n",
    "data_all = data_all[:,columns_tokeep]\n",
    "\n",
    "# remove labels that were used for concatenation\n",
    "mask = labels_all != \"init_label\"\n",
    "data = data_all[mask,:]\n",
    "labels = labels_all[mask]\n",
    "\n",
    "# save compiled data\n",
    "np.save(path_to + data, data)\n",
    "np.save(path_to + labels, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6274bd53ad02975ad3a1fc7c53468a83b5e714d6ade680c552f31a0436ec5d39"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
